{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics Of Natural Language Processsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentence=nlp(\"Hi There, Welcome To Natural Language Processing Basics, performed by Jinish Kanpara\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi INTJ 91 8206900633647566924\n",
      "There ADV 86 400\n",
      ", PUNCT 97 445\n",
      "Welcome VERB 100 428\n",
      "To ADP 85 443\n",
      "Natural PROPN 96 7037928807040764755\n",
      "Language PROPN 96 7037928807040764755\n",
      "Processing PROPN 96 7037928807040764755\n",
      "Basics PROPN 96 439\n",
      ", PUNCT 97 445\n",
      "performed VERB 100 8206900633647566924\n",
      "by ADP 85 401\n",
      "Jinish PROPN 96 7037928807040764755\n",
      "Kanpara PROPN 96 439\n"
     ]
    }
   ],
   "source": [
    "for items in Sentence:\n",
    "    print(items.text,items.pos_,items.pos,items.dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In here pos means parts of speech, i.e. which type of speech is it.\n",
    "#To elaborate the statement, we can see it's explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Basics"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sentence[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PROPN'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sentence[8].pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'proper noun'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(Sentence[8].pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = nlp(u'Although commmonly attributed to John Lennon from his song \"Beautiful Boy\", \\\n",
    "the phrase \"Life is what happens to us while we are making other plans\" was written by \\\n",
    "cartoonist Allen Saunders and published in Reader\\'s Digest in 1957, when Lennon was 17.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Life is what happens to us while we are making other plans\"\n"
     ]
    }
   ],
   "source": [
    "life_quote = doc3[16:30]\n",
    "print(life_quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(life_quote) #span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4 = nlp(u'This is the first sentence. This is another sentence. This is the last sentence.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first sentence.\n",
      "This is another sentence.\n",
      "This is the last sentence.\n"
     ]
    }
   ],
   "source": [
    "for sentences in doc4.sents:\n",
    "    print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc4[6].is_sent_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Statement=nlp(\"This is the statement to learn tokenization in Tableau\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This | is | the | statement | to | learn | tokenization | in | Tableau | "
     ]
    }
   ],
   "source": [
    "for tokens in Statement:\n",
    "    print(tokens.text,end = \" | \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Statement1 = nlp(\"Reliance Industries seems to be earning 200 million dollars more than TATA industries in India\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reliance Industries - ORG Companies, agencies, institutions, etc.\n",
      "200 million dollars - MONEY Monetary values, including unit\n",
      "TATA - ORG Companies, agencies, institutions, etc.\n",
      "India - GPE Countries, cities, states\n"
     ]
    }
   ],
   "source": [
    "for entities in Statement1.ents:\n",
    "    print(entities.text+\" - \"+entities.label_,spacy.explain(entities.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars\n",
      "insurance liability\n",
      "manufacturers\n"
     ]
    }
   ],
   "source": [
    "doc9 = nlp(u\"Autonomous cars shift insurance liability toward manufacturers.\")\n",
    "\n",
    "for chunk in doc9.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Statement2 = nlp(\"Hey, remember the time we went for cycling in Stanley park on 24th October 2018\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Hey, remember the time we went for cycling in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Stanley\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " park on \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    24th October 2018\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(Statement2,style = \"ent\",jupyter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_Stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_words = ['gathering','cycling','beautiful','fairly','astonishingly','promtly','goodness','running']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gathering--->gather\n",
      "cycling--->cycl\n",
      "beautiful--->beauti\n",
      "fairly--->fairli\n",
      "astonishingly--->astonishingli\n",
      "promtly--->promtli\n",
      "goodness--->good\n",
      "running--->run\n"
     ]
    }
   ],
   "source": [
    "for words in sample_words:\n",
    "    print(words+'--->'+P_Stemmer.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can certainly see in the above results that some of the words\n",
    "#such as \n",
    "# 1.promtly\n",
    "# 2.beautiful\n",
    "# 3. astonishingly\n",
    "#are not settled according to the needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_Stemmer = SnowballStemmer(language = \"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_words = ['gathering','cycling','beautiful','fairly','astonishingly','promtly','goodness','running']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gathering--->gather\n",
      "cycling--->cycl\n",
      "beautiful--->beauti\n",
      "fairly--->fair\n",
      "astonishingly--->astonish\n",
      "promtly--->promt\n",
      "goodness--->good\n",
      "running--->run\n"
     ]
    }
   ],
   "source": [
    "for words in sample_words:\n",
    "    print(words+'--->'+S_Stemmer.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results imporves here\n",
    "#1. astonishingly--->astonish\n",
    "#2. fairly--->fair\n",
    "#3. promtly--->promt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(u\"I am a runner running in a race because I love to run since I ran today\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t PRON \t 561228191312463089 \t -PRON-\n",
      "am \t VERB \t 10382539506755952630 \t be\n",
      "a \t DET \t 11901859001352538922 \t a\n",
      "runner \t NOUN \t 12640964157389618806 \t runner\n",
      "running \t VERB \t 12767647472892411841 \t run\n",
      "in \t ADP \t 3002984154512732771 \t in\n",
      "a \t DET \t 11901859001352538922 \t a\n",
      "race \t NOUN \t 8048469955494714898 \t race\n",
      "because \t ADP \t 16950148841647037698 \t because\n",
      "I \t PRON \t 561228191312463089 \t -PRON-\n",
      "love \t VERB \t 3702023516439754181 \t love\n",
      "to \t PART \t 3791531372978436496 \t to\n",
      "run \t VERB \t 12767647472892411841 \t run\n",
      "since \t ADP \t 10066841407251338481 \t since\n",
      "I \t PRON \t 561228191312463089 \t -PRON-\n",
      "ran \t VERB \t 12767647472892411841 \t run\n",
      "today \t NOUN \t 11042482332948150395 \t today\n"
     ]
    }
   ],
   "source": [
    "for words in doc1:\n",
    "    print(words.text,\"\\t\",words.pos_,\"\\t\",words.lemma,\"\\t\",words.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Why not create a function instead of using the same sequence everytime\n",
    "def primary_lemma_function(text):\n",
    "    for words in text:\n",
    "        print(f'{words.text:{12}} {words.pos_:{6}} {words.lemma:<{22}} {words.lemma_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I            PRON   561228191312463089     -PRON-\n",
      "am           VERB   10382539506755952630   be\n",
      "a            DET    11901859001352538922   a\n",
      "runner       NOUN   12640964157389618806   runner\n",
      "running      VERB   12767647472892411841   run\n",
      "in           ADP    3002984154512732771    in\n",
      "a            DET    11901859001352538922   a\n",
      "race         NOUN   8048469955494714898    race\n",
      "because      ADP    16950148841647037698   because\n",
      "I            PRON   561228191312463089     -PRON-\n",
      "love         VERB   3702023516439754181    love\n",
      "to           PART   3791531372978436496    to\n",
      "run          VERB   12767647472892411841   run\n",
      "since        ADP    10066841407251338481   since\n",
      "I            PRON   561228191312463089     -PRON-\n",
      "ran          VERB   12767647472892411841   run\n",
      "today        NOUN   11042482332948150395   today\n"
     ]
    }
   ],
   "source": [
    "primary_lemma_function(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make\n",
      "’s\n",
      "sometimes\n",
      "sixty\n",
      "these\n",
      "might\n",
      "again\n",
      "re\n",
      "several\n",
      "‘ll\n",
      "enough\n",
      "seemed\n",
      "anyhow\n",
      "every\n",
      "next\n",
      "sometime\n",
      "whether\n",
      "at\n",
      "becomes\n",
      "then\n",
      "formerly\n",
      "empty\n",
      "above\n",
      "other\n",
      "something\n",
      "others\n",
      "’m\n",
      "six\n",
      "go\n",
      "towards\n",
      "nobody\n",
      "but\n",
      "him\n",
      "eleven\n",
      "have\n",
      "another\n",
      "keep\n",
      "either\n",
      "yourself\n",
      "had\n",
      "if\n",
      "themselves\n",
      "can\n",
      "therefore\n",
      "up\n",
      "their\n",
      "same\n",
      "as\n",
      "‘ve\n",
      "else\n",
      "four\n",
      "us\n",
      "nowhere\n",
      "become\n",
      "ever\n",
      "his\n",
      "whence\n",
      "what\n",
      "due\n",
      "whole\n",
      "ours\n",
      "’ve\n",
      "whoever\n",
      "those\n",
      "among\n",
      "out\n",
      "thereafter\n",
      "doing\n",
      "he\n",
      "of\n",
      "various\n",
      "beforehand\n",
      "please\n",
      "while\n",
      "within\n",
      "whereafter\n",
      "was\n",
      "where\n",
      "between\n",
      "few\n",
      "myself\n",
      "is\n",
      "anyway\n",
      "three\n",
      "bottom\n",
      "often\n",
      "does\n",
      "being\n",
      "last\n",
      "i\n",
      "thereby\n",
      "'m\n",
      "from\n",
      "really\n",
      "her\n",
      "n't\n",
      "any\n",
      "around\n",
      "‘d\n",
      "rather\n",
      "whatever\n",
      "almost\n",
      "against\n",
      "too\n",
      "hereupon\n",
      "forty\n",
      "ourselves\n",
      "hereby\n",
      "such\n",
      "therein\n",
      "nevertheless\n",
      "yours\n",
      "me\n",
      "down\n",
      "quite\n",
      "its\n",
      "top\n",
      "has\n",
      "wherever\n",
      "ten\n",
      "here\n",
      "third\n",
      "still\n",
      "whom\n",
      "before\n",
      "both\n",
      "much\n",
      "many\n",
      "herein\n",
      "it\n",
      "somewhere\n",
      "itself\n",
      "should\n",
      "why\n",
      "put\n",
      "been\n",
      "anyone\n",
      "how\n",
      "the\n",
      "whereby\n",
      "very\n",
      "also\n",
      "my\n",
      "five\n",
      "yourselves\n",
      "twenty\n",
      "thence\n",
      "do\n",
      "made\n",
      "since\n",
      "yet\n",
      "full\n",
      "so\n",
      "along\n",
      "noone\n",
      "did\n",
      "whose\n",
      "during\n",
      "thereupon\n",
      "beside\n",
      "part\n",
      "thus\n",
      "namely\n",
      "none\n",
      "’re\n",
      "together\n",
      "on\n",
      "anywhere\n",
      "n‘t\n",
      "by\n",
      "'ve\n",
      "an\n",
      "than\n",
      "unless\n",
      "using\n",
      "each\n",
      "name\n",
      "wherein\n",
      "to\n",
      "herself\n",
      "whereupon\n",
      "some\n",
      "always\n",
      "there\n",
      "whenever\n",
      "'ll\n",
      "for\n",
      "could\n",
      "'re\n",
      "someone\n",
      "thru\n",
      "eight\n",
      "under\n",
      "may\n",
      "already\n",
      "’d\n",
      "mostly\n",
      "became\n",
      "see\n",
      "‘s\n",
      "besides\n",
      "'d\n",
      "toward\n",
      "get\n",
      "nor\n",
      "only\n",
      "cannot\n",
      "throughout\n",
      "'s\n",
      "further\n",
      "seems\n",
      "onto\n",
      "everything\n",
      "nine\n",
      "through\n",
      "all\n",
      "and\n",
      "across\n",
      "fifteen\n",
      "back\n",
      "most\n",
      "just\n",
      "everywhere\n",
      "hence\n",
      "no\n",
      "upon\n",
      "behind\n",
      "done\n",
      "neither\n",
      "latterly\n",
      "side\n",
      "becoming\n",
      "although\n",
      "about\n",
      "himself\n",
      "them\n",
      "are\n",
      "seeming\n",
      "elsewhere\n",
      "this\n",
      "perhaps\n",
      "take\n",
      "otherwise\n",
      "be\n",
      "because\n",
      "into\n",
      "own\n",
      "indeed\n",
      "fifty\n",
      "except\n",
      "nothing\n",
      "though\n",
      "afterwards\n",
      "first\n",
      "alone\n",
      "meanwhile\n",
      "we\n",
      "below\n",
      "‘m\n",
      "or\n",
      "after\n",
      "one\n",
      "somehow\n",
      "they\n",
      "via\n",
      "would\n",
      "will\n",
      "am\n",
      "mine\n",
      "a\n",
      "per\n",
      "more\n",
      "not\n",
      "without\n",
      "well\n",
      "say\n",
      "your\n",
      "’ll\n",
      "even\n",
      "that\n",
      "were\n",
      "hereafter\n",
      "whereas\n",
      "who\n",
      "never\n",
      "whither\n",
      "off\n",
      "once\n",
      "in\n",
      "must\n",
      "amongst\n",
      "however\n",
      "which\n",
      "our\n",
      "former\n",
      "amount\n",
      "beyond\n",
      "call\n",
      "moreover\n",
      "serious\n",
      "ca\n",
      "regarding\n",
      "over\n",
      "n’t\n",
      "least\n",
      "two\n",
      "‘re\n",
      "front\n",
      "latter\n",
      "until\n",
      "with\n",
      "twelve\n",
      "show\n",
      "when\n",
      "now\n",
      "hundred\n",
      "everyone\n",
      "anything\n",
      "move\n",
      "seem\n",
      "she\n",
      "give\n",
      "you\n",
      "less\n",
      "hers\n",
      "used\n"
     ]
    }
   ],
   "source": [
    "for items in nlp.Defaults.stop_words:\n",
    "    print(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab[\"becomes\"].is_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.Defaults.stop_words.add(\"bright\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab[\"bright\"].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
